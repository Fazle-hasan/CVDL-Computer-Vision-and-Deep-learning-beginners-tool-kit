{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Practical_8.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEZXVutz7nSF","outputId":"b3577420-1472-4bc9-b6b7-8a27b509ed0b","executionInfo":{"status":"ok","timestamp":1652348940952,"user_tz":-330,"elapsed":386,"user":{"displayName":"Fazel Hasan","userId":"12561225706966741704"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 0.6608483263015293\n","Loss: 0.00015650690213341546\n","Loss: 0.00015261634773374417\n","Loss: 0.00015113990403085308\n","Loss: 0.00014973352727383132\n","Loss: 0.00014839040062500827\n","Loss: 0.0001471053074918743\n","Loss: 0.00014587353445553962\n","Loss: 0.00014469081277401598\n","Loss: 0.00014355326774169555\n","Input: [[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: [[0.92]\n"," [0.86]\n"," [0.89]]\n","Loss: 0.00014245737467995042\n","\n","\n","Predicted Output: [[0.90311682]\n"," [0.87102892]\n"," [0.89454897]]\n"]}],"source":["import numpy as np\n","# X = (hours sleeping, hours studying), y = test score of the student\n","X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n","y = np.array(([92], [86], [89]), dtype=float)\n","\n","# scale units\n","X = X/np.amax(X, axis=0) #maximum of X array\n","y = y/100 # maximum test score is 100\n","\n","class NeuralNetwork(object):\n","    def __init__(self):\n","        #parameters\n","        self.inputSize = 2\n","        self.outputSize = 1\n","        self.hiddenSize = 3\n","        \n","        #weights\n","        self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n","        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n","        \n","    def feedForward(self, X):\n","        #forward propogation through the network\n","        self.z = np.dot(X, self.W1) #dot product of X (input) and first set of weights (3x2)\n","        self.z2 = self.sigmoid(self.z) #activation function\n","        self.z3 = np.dot(self.z2, self.W2) #dot product of hidden layer (z2) and second set of weights (3x1)\n","        output = self.sigmoid(self.z3)\n","        return output\n","        \n","    def sigmoid(self, s, deriv=False):\n","        if (deriv == True):\n","            return s * (1 - s)\n","        return 1/(1 + np.exp(-s))\n","    \n","    def backward(self, X, y, output):\n","        #backward propogate through the network\n","        self.output_error = y - output # error in output\n","        self.output_delta = self.output_error * self.sigmoid(output, deriv=True)\n","        \n","        self.z2_error = self.output_delta.dot(self.W2.T) #z2 error: how much our hidden layer weights contribute to output error\n","        self.z2_delta = self.z2_error * self.sigmoid(self.z2, deriv=True) #applying derivative of sigmoid to z2 error\n","        \n","        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input -> hidden) weights\n","        self.W2 += self.z2.T.dot(self.output_delta) # adjusting second set (hidden -> output) weights\n","        \n","    def train(self, X, y):\n","        output = self.feedForward(X)\n","        self.backward(X, y, output)\n","        \n","NN = NeuralNetwork()\n","\n","for i in range(1000): #trains the NN 1000 times\n","    if (i % 100 == 0):\n","        print(\"Loss: \" + str(np.mean(np.square(y - NN.feedForward(X)))))\n","    NN.train(X, y)\n","        \n","print(\"Input: \" + str(X))\n","print(\"Actual Output: \" + str(y))\n","print(\"Loss: \" + str(np.mean(np.square(y - NN.feedForward(X)))))\n","print(\"\\n\")\n","print(\"Predicted Output: \" + str(NN.feedForward(X)))"]}]}